{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "<img src=\"../images/logo.png\" width=150>\n",
    "\n",
    "在这节课上我们将学习线性回归。 我们将先理解它背后的数学基础原理再用python去实现它。 我们还将通过方法去讲解线性模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qHciBsX93ej"
   },
   "source": [
    "<img src=\"../images/linear.png\" width=250>\n",
    "\n",
    "$\\hat{y} = XW$\n",
    "\n",
    "*where*:\n",
    "* $\\hat{y}$ = 预测值 | $\\in \\mathbb{R}^{NX1}$ ($N$ 是样本的个数)\n",
    "* $X$ = 输入 | $\\in \\mathbb{R}^{NXD}$ ($D$ 是特征的个数)\n",
    "* $W$ = 权重 | $\\in \\mathbb{R}^{DX1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAgr7Grv9pb6"
   },
   "source": [
    "* **目标:**  通过线性模型的输入 $X$ 去预测 $\\hat{y}$。模型将会寻找一条最优的线使得我们的预测值和目标值最为接近。训练数据 $(X, y)$ 用来训练这个模型并且通过随机梯度下降(SGD)学习权重 $W$。\n",
    "* **优点:**\n",
    "  * 计算简单。\n",
    "  * 解释性强。\n",
    "  * 可用于连续（continuous）和无序的类别（categorical）特征。\n",
    "* **缺点:**\n",
    "  * 线性模型只能用于线性可分的数据(针对于分类任务).\n",
    "  * 但是通常来讲不会用于分类任务，仅仅用于回归问题。\n",
    "* **其他:** 当然你也可以使用线性回归去做二分类任务，如果预测出的连续数值高于一个阈值它就属于一个特定的分类。但是我们在未来的课程中将会介绍可用于做二分类任务更好的模型，所以我们本次课程只会集中在怎么用线性回归去做回归任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP7XD24-09Io"
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "476yPgTM1BKJ"
   },
   "source": [
    "*步骤*: \n",
    "1. 随机初始化模型的权重$W$。\n",
    "2. 将输入值 $X$ 传入模型并且得到预测值$\\hat{y}$。\n",
    "3. 通过损失函数来计算预测值$\\hat{y}$和真实值$\\hat{y}$之间的差距，从而得到损失值$J$。普遍在线性回归中用到的损失函数是均方误差(MSE)。这个函数计算出预测值和真实值之间的差距的平方($\\frac{1}{2}$ 没有数学意义，只是在求导的时候可以正好和平方抵消，方便计算)。\n",
    "  * $MSE = J(\\theta) = \\frac{1}{2}\\sum_{i}(\\hat{y}_i - y_i)^2$\n",
    "4. 计算出对于模型权重的损失梯度$J(\\theta)$\n",
    "  * $J(\\theta) = \\frac{1}{2}\\sum_{i}(\\hat{y}_i - y_i)^2 = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 $\n",
    "  * $\\frac{\\partial{J}}{\\partial{W}} = X(\\hat{y} - y)$\n",
    "4. 我们使用学习率$\\alpha$和一个优化方法(比如随机梯度下降)，通过反向传播来更新权重$W$。 一个简单的比方就是梯度可以告诉你在哪个方向上增加数值，然后通过减法来使得损失值$J(\\theta)$越来越小。\n",
    "  * $W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "5. 重复2 - 4步直到模型表现最好（也可以说直到损失收敛）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvJKjkMeJP4Q"
   },
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuPl9qlSJTIY"
   },
   "source": [
    "我们将自己创建一些假数据应用在线性回归上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRXD7LqVJZ43"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFsKg-Z6IWqG"
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"../data/sample_data.csv\",\n",
    "    num_samples=100,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "# 设置随机种子来保证实验结果的可重复性。\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWux2lcoIWss"
   },
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "def generate_data(num_samples):\n",
    "    X = np.array(range(num_samples))\n",
    "    y = 3.65*X + 10\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2mb2SjSQIWvF",
    "outputId": "3aa66ef6-3c88-40fd-f53a-a77f93c8e052"
   },
   "outputs": [],
   "source": [
    "# 生成随机数据\n",
    "X, y = generate_data(args.num_samples)\n",
    "data = np.vstack([X, y]).T\n",
    "df = pd.DataFrame(data, columns=['X', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "6LwVVOkiLfBN",
    "outputId": "ab2ecddb-bbeb-4117-d1cf-29312846da16"
   },
   "outputs": [],
   "source": [
    "# 画散点图\n",
    "plt.title(\"Generated data\")\n",
    "plt.scatter(x=df[\"X\"], y=df[\"y\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwn29SjK-XCg"
   },
   "source": [
    "# Scikit-learn 实现方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kSEp8MY-y9C"
   },
   "source": [
    "**注意**: `LinearRegression`类在Scikit-learn中使用的是正规方程法来做的拟合。然而，我们将会使用Scikit-learn中的随机梯度下降`SGDRegressor`类来拟合数据。我们使用这个优化方法是因为在未来的几节课程中我们也会使用到它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKmBKodpgHEE"
   },
   "outputs": [],
   "source": [
    "# 调包\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WuUQwD72NVAE",
    "outputId": "c46cd73c-7ca4-4d57-ee4b-0fc636f5cde5"
   },
   "outputs": [],
   "source": [
    "# 划分数据到训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"X\"].values.reshape(-1, 1), df[\"y\"], test_size=args.test_size, \n",
    "    random_state=args.seed)\n",
    "print (\"X_train:\", X_train.shape)\n",
    "print (\"y_train:\", y_train.shape)\n",
    "print (\"X_test:\", X_test.shape)\n",
    "print (\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJVs6JF7trja"
   },
   "source": [
    "我们需要标准化我们的数据（零均值和单位方差），以便正确使用SGD并在速度上优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VlOYPD5GRjRC",
    "outputId": "9d63c0d3-da44-487c-fb8c-c9879f2d97d3"
   },
   "outputs": [],
   "source": [
    "# 标准化训练集数据 (mean=0, std=1) \n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train.values.reshape(-1,1))\n",
    "\n",
    "# 在训练集和测试集上进行标准化操作\n",
    "standardized_X_train = X_scaler.transform(X_train)\n",
    "standardized_y_train = y_scaler.transform(y_train.values.reshape(-1,1)).ravel()\n",
    "standardized_X_test = X_scaler.transform(X_test)\n",
    "standardized_y_test = y_scaler.transform(y_test.values.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "# 检查\n",
    "print (\"mean:\", np.mean(standardized_X_train, axis=0), \n",
    "       np.mean(standardized_y_train, axis=0)) # mean 应该是 ~0\n",
    "print (\"std:\", np.std(standardized_X_train, axis=0), \n",
    "       np.std(standardized_y_train, axis=0))   # std 应该是 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiE3oLCkOCEa"
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "lm = SGDRegressor(loss=\"squared_loss\", penalty=\"none\", max_iter=args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sGH_pQaDOb49",
    "outputId": "ddfff892-314a-4d19-d8a3-549b5e6c555a"
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "lm.fit(X=standardized_X_train, y=standardized_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA8VkVVGPkTr"
   },
   "outputs": [],
   "source": [
    "# 预测 (还未标准化)\n",
    "pred_train = (lm.predict(standardized_X_train) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\n",
    "pred_test = (lm.predict(standardized_X_test) * np.sqrt(y_scaler.var_)) + y_scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8Ws-khqJuNr"
   },
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2pha3VRWd2D"
   },
   "source": [
    "有很多种方法可以来评估我们模型表现的好坏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abGgfBbLVjJ_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RKm8IiP7O66e",
    "outputId": "4fd58928-36da-4d96-9222-9658e1a0bd58"
   },
   "outputs": [],
   "source": [
    "# 训练和测试集上的均方误差 MSE\n",
    "train_mse = np.mean((y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((y_test - pred_test) ** 2)\n",
    "print (\"train_MSE: {0:.2f}, test_MSE: {1:.2f}\".format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TegkJM2-YKEq"
   },
   "source": [
    "除了使用MSE，如果我们只有一个特征向量，我们可以把他们可视化出来直观的评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "gH5N-U7YQVgn",
    "outputId": "4a1ce429-18c5-45e8-e701-2cc7affcb94b"
   },
   "outputs": [],
   "source": [
    "# 图例大小\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# 画出训练数据\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plt.scatter(X_train, y_train, label=\"y_train\")\n",
    "plt.plot(X_train, pred_train, color=\"red\", linewidth=1, linestyle=\"-\", label=\"lm\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# 画出测试数据\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plt.scatter(X_test, y_test, label=\"y_test\")\n",
    "plt.plot(X_test, pred_test, color=\"red\", linewidth=1, linestyle=\"-\", label=\"lm\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# 显示图例\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAP1EoQi86XB"
   },
   "source": [
    "# 推论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "K2yfNk3d8-Vj",
    "outputId": "aef548b1-d0a1-4481-a9fe-4c975f278a3f"
   },
   "outputs": [],
   "source": [
    "# 传入我们自己的输入值\n",
    "X_infer = np.array((0, 1, 2), dtype=np.float32)\n",
    "standardized_X_infer = X_scaler.transform(X_infer.reshape(-1, 1))\n",
    "pred_infer = (lm.predict(standardized_X_infer) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\n",
    "print (pred_infer)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHH0fYp_BYC5"
   },
   "source": [
    "# 可解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhXo8CbPBZ-G"
   },
   "source": [
    "线性回归有很强的可解释性。每一个特征都有一个系数来控制对输出值y的影响大小。我们可以这样解释这个系数: 如果我们把x增加1, 我们将把y增加 $W$ (~3.65)。\n",
    "\n",
    "**注意**: 因为我们在梯度下降时标准化了我们的输入和输出，我们需要对我们的系数和截距做一个反标准化。过程可见下方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JZxnrDuCBbK9",
    "outputId": "adbe06d5-449a-4aa0-ebbb-ef5fdd5ef9e9"
   },
   "outputs": [],
   "source": [
    "# 未标准化系数\n",
    "coef = lm.coef_ * (y_scaler.scale_/X_scaler.scale_)\n",
    "intercept = lm.intercept_ * y_scaler.scale_ + y_scaler.mean_ - np.sum(coef*X_scaler.mean_)\n",
    "print (coef) # ~3.65\n",
    "print (intercept) # ~10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVmIP13u9s33"
   },
   "source": [
    "### 非标准化系数的证明：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViDPSLbR9v4B"
   },
   "source": [
    "注意我们的X和y都已经标准化了。\n",
    "\n",
    "$\\frac{\\mathbb{E}[y] - \\hat{y}}{\\sigma_y} = W_0 + \\sum_{j=1}^{k}W_jz_j$\n",
    "\n",
    "$z_j = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$ \\hat{y}_{scaled} = \\frac{\\hat{y}_{unscaled} - \\bar{y}}{\\sigma_y} = \\hat{W_0} + \\sum_{j=1}^{k} \\hat{W}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n",
    "\n",
    "$\\hat{y}_{unscaled} = \\hat{W}_0\\sigma_y + \\bar{y} - \\sum_{j=1}^{k} \\hat{W}_j(\\frac{\\sigma_y}{\\sigma_j})\\bar{x}_j + \\sum_{j=1}^{k}(\\frac{\\sigma_y}{\\sigma_j})x_j $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rToCXKqeJcvj"
   },
   "source": [
    "# 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4GFv8xRJmOZ"
   },
   "source": [
    "正规化有助于减少过拟合。下方是L2正则化(ridge regression)。有很多正则化的方法他们都可以使我们的模型减少过拟合。对于L2正则化, 我们会减小那些值很大的权重。数值很大的权重将会使模型更加看中它们的特征，但是我们希望的是模型会公平的对待所有的特征而不是仅仅权重很大的几个。 当然还有其他的正则化方法比如L1(lasso regression)，它对于我们想创建更加稀疏的数据模型有好处，因为它会使得一些权重变成0，或者我们可以结合L2和L1正则化方法。\n",
    "\n",
    "**注意**: 正则化不仅仅用于线性回归。它可以用于任何常规模型以及我们以后将会学到的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_OcpRxF-Oj7"
   },
   "source": [
    "* $ J(\\theta) = = \\frac{1}{2}\\sum_{i}(X_iW - y_i)^2 + \\frac{\\lambda}{2}\\sum\\sum W^2$\n",
    "* $ \\frac{\\partial{J}}{\\partial{W}}  = X (\\hat{y} - y) + \\lambda W $\n",
    "* $W = W- \\alpha\\frac{\\partial{J}}{\\partial{W}}$\n",
    "where:\n",
    "  * $\\lambda$ 是正则化系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHaazL9f8QZX"
   },
   "outputs": [],
   "source": [
    "# 初始化带有L2正则化的模型\n",
    "lm = SGDRegressor(loss=\"squared_loss\", penalty='l2', alpha=1e-2, \n",
    "                  max_iter=args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VTIUZLbGZP4e",
    "outputId": "e284d26b-6091-4ce9-b40c-5b9d675f0837"
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "lm.fit(X=standardized_X_train, y=standardized_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORwkUqcuZhbX"
   },
   "outputs": [],
   "source": [
    "# 预测 (还未标准化)\n",
    "pred_train = (lm.predict(standardized_X_train) * np.sqrt(y_scaler.var_)) + y_scaler.mean_\n",
    "pred_test = (lm.predict(standardized_X_test) * np.sqrt(y_scaler.var_)) + y_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IWCvYxBxZhd5",
    "outputId": "a38bb230-eb97-43c1-d43d-8dead30dedcb"
   },
   "outputs": [],
   "source": [
    "# 训练集和测试集的MSE\n",
    "train_mse = np.mean((y_train - pred_train) ** 2)\n",
    "test_mse = np.mean((y_test - pred_test) ** 2)\n",
    "print (\"train_MSE: {0:.2f}, test_MSE: {1:.2f}\".format(\n",
    "    train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdNX2W5eh2ma"
   },
   "source": [
    "正则化对于我们在做的这个数据帮助很少，因为我们在创建数据的时候用的就是一个线性的函数。但是对于现实中的数据，正则化就可以帮助我们构建更好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "C2mrVS4UZp3Q",
    "outputId": "5ba6159b-0791-4093-9ed9-8925386edf36"
   },
   "outputs": [],
   "source": [
    "# 未标准化系数\n",
    "coef = lm.coef_ * (y_scaler.scale_/X_scaler.scale_)\n",
    "intercept = lm.intercept_ * y_scaler.scale_ + y_scaler.mean_ - (coef*X_scaler.mean_)\n",
    "print (coef) # ~3.65\n",
    "print (intercept) # ~10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V74lNFE5v5pQ"
   },
   "source": [
    "# 类别变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r6Xhyg7v5vX"
   },
   "source": [
    "在我们的例子中，特征用的是连续的数值，那么假设我们要用类别的特征变量呢？一种选择就是使用独热编码来处理类别变量，这种方法用Pandas很容易实现，你可以用和上面相同的步骤来训练你的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "unhcIOfMxQEQ",
    "outputId": "ecf36ce0-28af-4381-b61b-2592a3c1b289"
   },
   "outputs": [],
   "source": [
    "# 创建类别特征\n",
    "cat_data = pd.DataFrame(['a', 'b', 'c', 'a'], columns=['favorite_letter'])\n",
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "m4eQmJdrxQGr",
    "outputId": "247aaac2-afcb-4899-e415-91d3fe169fbf"
   },
   "outputs": [],
   "source": [
    "dummy_cat_data = pd.get_dummies(cat_data) #独热编码 one-hot encoding，与dummy变量不同要注意。\n",
    "dummy_cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5R8x-KyiBWJ"
   },
   "source": [
    "现在你可以拼接上连续特征变量来训练线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVOXoCRsokzp"
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4c7ttuUwfeLA"
   },
   "source": [
    "- 多项式回归\n",
    "- 一个简单的用正规方程的例子(sklearn.linear_model.LinearRegression)来分析优点和缺点，并且和随机梯度下降线性回归做对比。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_Linear_Regression",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
