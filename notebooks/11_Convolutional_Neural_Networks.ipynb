{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# 卷积神经网络 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "<img src=\"../images/logo.png\" width=150>\n",
    "\n",
    "本课程中，我们将学习基本的卷积神经网络（CNN），并应用到自然语言（NLP）处理中。CNN 常用于图像处理，并且有很多[例子](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)。但我们专注于将它用于文本数据，也有惊人的效果。\n",
    "\n",
    "In this lesson we will learn the basics of Convolutional Neural Networks (CNNs) applied to text for natural language processing (NLP) tasks. CNNs are traditionally used on images and there are plenty of [tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) that cover this. But we're going to focus on using CNN on text data which yields amazing results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# 总览 Overview\n",
    "\n",
    "[论文](https://arxiv.org/abs/1510.03820)中图表展示了句子中的词如何进行一维卷积计算。\n",
    "\n",
    "The diagram below from this [paper](https://arxiv.org/abs/1510.03820) shows how 1D convolution is applied to the words in a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziGJNhiQeiGN"
   },
   "source": [
    "<img src=\"../images/cnn_text.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "* **目的：**  检测输入数据的空间结构。Detect spatial substructure from input data.\n",
    "* **优点：** \n",
    "  * 权重数量小（共享）Small number of weights (shared)\n",
    "  * 可并行的 Parallelizable\n",
    "  * 检测空间结构（特征提取）Detects spatial substrcutures (feature extractors)\n",
    "  * 通过滤波器可翻译的 Interpretable via filters\n",
    "  * 可用于图像、文本、时间序列等 Used for in images/text/time-series etc.\n",
    "* **缺点：**\n",
    "  * 很多超参数（核大小，步幅等）Many hyperparameters (kernel size, strides, etc.)\n",
    "  * 输入宽度必须相同（图像维度，文本长度等）Inputs have to be of same width (image dimensions, text length, etc.)\n",
    "* **其他：** \n",
    "  * 很多深度 CNN 框架持续更新 SOTA 性能  Lot's of deep CNN architectures constantly updated for SOTA performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nCsZGyWhI9f"
   },
   "source": [
    "# 滤波器 Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxpgRzIjiVHv"
   },
   "source": [
    "CNN 的核心是滤波器（权重，核等），它通过对输入进行卷积计算，提取相关特征。虽然滤波器是随机初始化的，但可以学习从输入选取有意义的特征，有助于优化目标。我们将通过不寻常的方式介绍 CNN，聚焦应用于二维文本数据。每个输入有多个词组成，我们使用独热编码（one-hot encoder）表示每个词，得到一个二维输入。每个滤波器代表一个特征，我们将应用滤波器到其他输入，捕获相同特征。这也称为参数共享。\n",
    "\n",
    "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevant features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as one-hot encoded vector which gives us our 2D input. The intuition here is that each filter represents a feature and we will use this filter on other inputs to capture the same feature. This is known as parameter sharing.\n",
    "\n",
    "<img src=\"../images/conv.gif\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1kTABJyYj91S",
    "outputId": "5f7c9497-63bf-46e3-e01b-74b5902ae569"
   },
   "outputs": [],
   "source": [
    "# Loading PyTorch library\n",
    "# !pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz9D2rrdmSl9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q1FiiIHXjI_"
   },
   "source": [
    "我们的输入是一组二维文本数据。设定输入有64个样本，其中每个样本有8个词，每个词用一个长度为10的数组（词汇量为10的独热编码）表示。这样输入的体积为（64，8，10）。[Python CNN 模块](https://pytorch.org/docs/stable/nn.html#convolution-functions)倾向于输入的通道维度（本例中是一个独热编码向量）在第2个位置，所以我们的输入维度是（64，10，8）。\n",
    "\n",
    "Our inputs are a batch of 2D text data. Let's make an input with 64 samples, where each sample has 8 words and each word is represented by a array of 10 values (one hot encoded with vocab size of 10). This gives our inputs the size (64, 8, 10). The [PyTorch CNN modules](https://pytorch.org/docs/stable/nn.html#convolution-functions) prefer inputs to have the channel dim (one hot vector dim in our case) to be in the second position, so our inputs are of shape (64, 10, 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFfYwCcjZj79"
   },
   "source": [
    "<img src=\"../images/cnn_text1.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b6G2nBvOxR-e",
    "outputId": "00f935dc-b4be-472c-8a9c-a3dc608955e9"
   },
   "outputs": [],
   "source": [
    "# Assume all our inputs have the same # of words\n",
    "batch_size = 64\n",
    "sequence_size = 8 # words per input\n",
    "one_hot_size = 10 # vocab size (num_input_channels)\n",
    "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
    "print(\"Size: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJmtay_UZohM"
   },
   "source": [
    "使用滤波器对输入进行卷积运算。为了简单一点，我们使用5个（1，2）的滤波器，深度和通道数相同（独热编码大小）。这样，滤波器维度是（5，2，10）。但是如上所述， PyTorch 更倾向于通道维度在第2个位置，所以滤波器维度是（5，10，2）。\n",
    "\n",
    "We want to convolve on this input using filters. For simplicity we will use just 5 filters that is of size (1, 2) and has the same depth as the number of channels (one_hot_size). This gives our filter a shape of (5, 2, 10) but recall that PyTorch CNN modules prefer to have the channel dim (one hot vector dim in our case) to be in the second position so the filter is of shape (5, 10, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJF0l88qb-21"
   },
   "source": [
    "<img src=\"../images/cnn_text2.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WMK2TzgDxR8B",
    "outputId": "32609363-1d3d-4437-c709-57bbac524064"
   },
   "outputs": [],
   "source": [
    "# Create filters for a conv layer\n",
    "out_channels = 5 # of filters\n",
    "kernel_size = 2 # filters size 2\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "print(\"Size: {}\".format(conv1.weight.shape))\n",
    "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
    "print(\"Padding: {}\".format(conv1.padding[0]))\n",
    "print(\"Stride: {}\".format(conv1.stride[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAcYxhDIbeWE"
   },
   "source": [
    "我们在输入上使用这个滤波器，得到（64，5，7）的输出。64是样本大小，5是通道维度（因为使用了5个滤波器），7是卷积输出，因为：\n",
    "\n",
    "$\\frac{W - F + 2P}{S} + 1 = \\frac{8 - 2 + 2(0)}{1} + 1 = 7$\n",
    "\n",
    "其中：\n",
    "  * W：输入宽度\n",
    "  * F：滤波器大小\n",
    "  * P：填充\n",
    "  * S：步幅\n",
    "  \n",
    "When we apply this filter on our inputs, we receive an output of shape (64, 5, 7). We get 64 for the batch size, 5 for the channel dim because we used 5 filters and 7 for the conv outputs because:\n",
    "\n",
    "$\\frac{W - F + 2P}{S} + 1 = \\frac{8 - 2 + 2(0)}{1} + 1 = 7$\n",
    "\n",
    "where:\n",
    "  * W: width of each input\n",
    "  * F: filter size\n",
    "  * P: padding\n",
    "  * S: stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2c_KKtP4hrJx"
   },
   "source": [
    "<img src=\"../images/cnn_text3.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yjxtrM89xR5a",
    "outputId": "d7787d5f-8450-4cdc-b234-03b68452c739"
   },
   "outputs": [],
   "source": [
    "# Convolve using filters\n",
    "conv_output = conv1(x)\n",
    "print(\"Size: {}\".format(conv_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwTtF7bBuZvF"
   },
   "source": [
    "# 池化 Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXBbKPs1ua9G"
   },
   "source": [
    "对输入进行卷积计算的结果是一个特征映射。因为卷积和重叠，特征映射有很多冗余信息。池化是一种降维的方式。池化可以是一定接收域的最大值，平局值等。\n",
    "\n",
    "The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field.\n",
    "\n",
    "<img src=\"../images/pool.jpeg\" width=450>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VCag6lk2mSwU",
    "outputId": "d60fdc17-1791-41ba-f7ae-86e78553e20a"
   },
   "outputs": [],
   "source": [
    "# Max pooling\n",
    "kernel_size = 2\n",
    "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "pool_output = pool1(conv_output)\n",
    "print(\"Size: {}\".format(pool_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_e4QRFwvTt8"
   },
   "source": [
    "$\\frac{W-F}{S} + 1 = \\frac{7-2}{2} + 1 =  \\text{floor }(2.5) + 1 = 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9rL1EWIfi-y"
   },
   "source": [
    "# CNN 文本处理 CNNs on text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWtHDOJgHZvk"
   },
   "source": [
    "我们正在用卷积神经网络处理文本数据，进行词级别的卷积计算，获取有用的n-grams（一种语言模型）。\n",
    "\n",
    "你可以把这个配置用于[时间序列](https://arxiv.org/abs/1807.10707) 数据或者与其他神经网络相结合。对于文本数据，我们会建立不同大小的滤波器，（1，2），（1，3）和（1，4），他们用于不同 n-gram 的特征选取器。把输出拼接起来，输入到一个全连接网络。本例中，我们使用字符级的一维卷积。在 [embeddings notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/12_Embeddings.ipynb)，我们在词级中应用了一维卷积。\n",
    "\n",
    "**词嵌入**：捕获相邻词汇的临时联系，使得相似单词有相似意义。\n",
    "\n",
    "**字符嵌入**：创建一个字符级别映射。例如\"toy\"和“toys”彼此接近。\n",
    "\n",
    "We're going use convolutional neural networks on text data which typically involves convolving on the character level representation of the text to capture meaningful n-grams. \n",
    "\n",
    "You can easily use this set up for [time series](https://arxiv.org/abs/1807.10707) data or [combine it](https://arxiv.org/abs/1808.04928) with other networks. For text data, we will create filters of varying kernel sizes (1,2), (1,3), and (1,4) which act as feature selectors of varying n-gram sizes. The outputs are concated and fed into a fully-connected layer for class predictions. In our example, we will be applying 1D convolutions on letter in a word. In the [embeddings notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/12_Embeddings.ipynb), we will apply 1D convolutions on words in a sentence.\n",
    "\n",
    "**Word embeddings**: capture the temporal correlations among\n",
    "adjacent tokens so that similar words have similar representations. Ex. \"New Jersey\" is close to \"NJ\" is close to \"Garden State\", etc.\n",
    "\n",
    "**Char embeddings**: create representations that map words at a character level. Ex. \"toy\" and \"toys\" will be close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVBZxbaAtS9u"
   },
   "source": [
    "# 配置 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8QSdEcDtXUs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VADCXjMwtXYN"
   },
   "outputs": [],
   "source": [
    "# Set Numpy and PyTorch seeds\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mpiCYECstXbT",
    "outputId": "0ef00d64-963e-4e23-cbea-d2e8187f5553"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"../data/surnames.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"../output/names\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    num_epochs=20,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    num_filters=100,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Create save dir\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# Expand filepaths\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptb4hJ4Bw8YU"
   },
   "source": [
    "# 数据 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNxZQUqfmS0B"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBdQpUTQtMgu"
   },
   "outputs": [],
   "source": [
    "# # Upload data from GitHub to notebook's local drive\n",
    "# url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# html = response.read()\n",
    "# with open(args.data_file, 'wb') as fp:\n",
    "#     fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6PYCeGrStMj7",
    "outputId": "849f86d5-1bdc-404f-bb45-44b3532a5069"
   },
   "outputs": [],
   "source": [
    "# Raw data\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "pbfVM-YatMnD",
    "outputId": "58746131-868b-4ab6-cef3-147f137e04e6"
   },
   "outputs": [],
   "source": [
    "# Split by nationality\n",
    "by_nationality = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_nationality[row.nationality].append(row.to_dict())\n",
    "for nationality in by_nationality:\n",
    "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdGOoKFjtMpz"
   },
   "outputs": [],
   "source": [
    "# Create split data\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DyDwlzzKtMsz",
    "outputId": "ba9fc7a2-819f-4889-941b-05b2868cb342"
   },
   "outputs": [],
   "source": [
    "# df with split datasets\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "17aHMQOwtMvh",
    "outputId": "eba52fd0-e276-459c-de8a-633f6ed9e05a"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.surname = split_df.surname.apply(preprocess_text)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nZBgfQTuAA8"
   },
   "source": [
    "# 词汇表 Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeRVQlRZuBgA"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "\n",
    "        # Token to index\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "        \n",
    "        # Add unknown token\n",
    "        self.add_unk = add_unk\n",
    "        self.unk_token = unk_token\n",
    "        if self.add_unk:\n",
    "            self.unk_index = self.add_token(self.unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx,\n",
    "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.add_unk:\n",
    "            index = self.token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            index =  self.token_to_idx[token]\n",
    "        return index\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bH8LMH9wuBi9",
    "outputId": "8293745c-751e-40d9-9993-78e84093e203"
   },
   "outputs": [],
   "source": [
    "# Vocabulary instance\n",
    "nationality_vocab = Vocabulary(add_unk=False)\n",
    "for index, row in df.iterrows():\n",
    "    nationality_vocab.add_token(row.nationality)\n",
    "print (nationality_vocab) # __str__\n",
    "index = nationality_vocab.lookup_token(\"English\")\n",
    "print (index)\n",
    "print (nationality_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57a1lzHPuHHm"
   },
   "source": [
    "# 向量化 Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwS5BEV-uBlt"
   },
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        one_hot_matrix_size = (len(surname), len(self.surname_vocab))\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[position_index][character_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "    \n",
    "    def unvectorize(self, one_hot_matrix):\n",
    "        len_name = len(one_hot_matrix)\n",
    "        indices = np.zeros(len_name)\n",
    "        for i in range(len_name):\n",
    "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
    "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
    "        return surname\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        surname_vocab = Vocabulary(add_unk=True)\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        # Create vocabularies\n",
    "        for index, row in df.iterrows():\n",
    "            for letter in row.surname: # char-level tokenization\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zq7RoFAXuBo9",
    "outputId": "3555b683-841a-4e2a-a246-c8ae4cb74636"
   },
   "outputs": [],
   "source": [
    "# Vectorizer instance\n",
    "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
    "print (vectorizer.surname_vocab)\n",
    "print (vectorizer.nationality_vocab)\n",
    "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
    "print (np.shape(vectorized_surname))\n",
    "print (vectorized_surname)\n",
    "print (vectorizer.unvectorize(vectorized_surname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwQ8MNp5ZfeG"
   },
   "source": [
    "**Note**: Unlike the bagged ont-hot encoding method in the MLP notebook, we are able to preserve the semantic structure of the surnames. We are able to use one-hot encoding here because we are using characters but when we process text with large vocabularies, this method simply can't scale. We'll explore embedding based methods in subsequent notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mnf7gXgKuOgp"
   },
   "source": [
    "# 数据集 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYqzM53fuBrf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjolk855uPrA"
   },
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # Data splits\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights (for imbalances)\n",
    "        class_counts = df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, df):\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, df, vectorizer_filepath):\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=True, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "hvy-CJVSuPuS",
    "outputId": "878b8447-2345-4918-d621-182583e88b7e"
   },
   "outputs": [],
   "source": [
    "# Dataset instance\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(split_df)\n",
    "print (dataset) # __str__\n",
    "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XY0CqM2Rd3Im"
   },
   "source": [
    "# 模型 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWGpAzKPd32f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7Q0_nkjd30L"
   },
   "outputs": [],
   "source": [
    "class SurnameModel(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
    "        super(SurnameModel, self).__init__()\n",
    "        \n",
    "        # Conv weights\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "       \n",
    "        # FC weights\n",
    "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        # Conv outputs\n",
    "        z = [conv(x) for conv in self.conv]\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
    "        z = [F.relu(zz) for zz in z]\n",
    "        \n",
    "        # Concat conv outputs\n",
    "        z = torch.cat(z, 1)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # FC layer\n",
    "        y_pred = self.fc1(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XlJwSKQkL_C"
   },
   "source": [
    "# 训练 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rh_1heUNSUYN"
   },
   "source": [
    "**填充：** 输入必须有相同的维度。我们的向量器把输入向量化，但是特殊的批次中，我们可能碰到不同大小的输入。解决方法是找到最长的输入，把其他的填充到这个长度。通常，数量最小的输入批次，用0向量填充。\n",
    "\n",
    "我们使用 Trainer 类中的 pad_seq 函数，由 collate_fn 触发，传递到 Dataset 类中的 generate_batches 函数。基本上，批次生成器产生样本，我们使用 collate_fn 确定最大的输入，填充批次中的其他输入，得到一个统一的输入维度。\n",
    "\n",
    "**Padding:** the inputs in a particular batch must all have the same shape. Our vectorizer converts the tokens into a vectorizer form but in a particular batch, we can have inputs of various sizes. The solution is to determine the longest input in a particular batch and pad all the other inputs to match that length. Usually, the smaller inputs in the batch are padded with zero vectors. \n",
    "\n",
    "We do this using the pad_seq function in the Trainer class which is invoked by the collate_fn which is passed to generate_batches function in the Dataset class. Essentially, the batch generater generates samples into a batch and we use the collate_fn to determine the largest input and pad all the other inputs in the batch to get a uniform input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLLmIuKRkNYW"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sV-Dc_5ykNgS"
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'done_training': False,\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # Verbose\n",
    "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # Save one model at least\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # Save model if performance improved\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # If loss worsened\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # Update step\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # Loss decreased\n",
    "            else:\n",
    "                # Save the best model\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # Reset early stopping step\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # Stop early ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros((length, len(self.dataset.vectorizer.surname_vocab)),\n",
    "                          dtype=np.int64)\n",
    "        for i in range(len(seq)):\n",
    "            vector[i] = seq[i]\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # Make a deep copy\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"surname\": [], \"nationality\": []}\n",
    "        \n",
    "        # Get max sequence length\n",
    "        max_seq_len = max([len(sample[\"surname\"]) for sample in batch_copy])\n",
    "        \n",
    "        # Pad\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            seq = sample[\"surname\"]\n",
    "            nationality = sample[\"nationality\"]\n",
    "            padded_seq = self.pad_seq(seq, max_seq_len)\n",
    "            processed_batch[\"surname\"].append(padded_seq)\n",
    "            processed_batch[\"nationality\"].append(nationality)\n",
    "            \n",
    "        # Convert to appropriate tensor types\n",
    "        processed_batch[\"surname\"] = torch.FloatTensor(\n",
    "            processed_batch[\"surname\"]) # need float for conv operations\n",
    "        processed_batch[\"nationality\"] = torch.LongTensor(\n",
    "            processed_batch[\"nationality\"])\n",
    "        \n",
    "        return processed_batch    \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # Iterate over train dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = self.model(batch_dict['surname'])\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute gradients using loss\n",
    "                loss.backward()\n",
    "\n",
    "                # use optimizer to take a gradient step\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0; set eval mode on\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # initialize batch generator, set loss and acc to 0; set eval mode on\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "            # compute the loss\n",
    "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # Figure size\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # Show plots\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        self.train_state[\"done_training\"] = True\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "OkeOQRwckNd1",
    "outputId": "77032240-5a41-4461-8767-89827357f6f7"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(split_df)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                     num_output_channels=args.num_filters,\n",
    "                     num_classes=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "3JJdOO4ZkNb3",
    "outputId": "c80d4f6a-a05e-4f58-fb4f-91ccd95f68a3"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "0QLZfEyznVpT",
    "outputId": "67d28ed8-2252-4947-ffc5-71c5c4f4fedd"
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BWGzMSaBnYMb",
    "outputId": "9fb5ed9a-8da0-4fc4-df7b-87a0ce67796a"
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5672VEginYnY"
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HN1g2vP3nad_"
   },
   "source": [
    "# 预测 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Myr8QQjKnZ7k"
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.vectorizer = vectorizer\n",
    "        self.device = device\n",
    "  \n",
    "    def predict_nationality(self, dataset):\n",
    "        # Batch generator\n",
    "        batch_generator = dataset.generate_batches(\n",
    "            batch_size=len(dataset), shuffle=False, device=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Predict\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred =  self.model(batch_dict['surname'], apply_softmax=True)\n",
    "\n",
    "            # Top k nationalities\n",
    "            y_prob, indices = torch.topk(y_pred, k=len(self.vectorizer.nationality_vocab))\n",
    "            probabilities = y_prob.detach().to('cpu').numpy()[0]\n",
    "            indices = indices.detach().to('cpu').numpy()[0]\n",
    "\n",
    "            results = []\n",
    "            for probability, index in zip(probabilities, indices):\n",
    "                nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
    "                results.append({'nationality': nationality, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VVn_zxkRcbf"
   },
   "outputs": [],
   "source": [
    "# Load vectorizer\n",
    "with open(args.vectorizer_file) as fp:\n",
    "    vectorizer = SurnameVectorizer.from_serializable(json.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "Wx46FK2YRchi",
    "outputId": "26a09df1-1548-469e-df7f-6613a644512c"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                     num_output_channels=args.num_filters,\n",
    "                     num_classes=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZE2Ov4xRcfq"
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "inference = Inference(model=model, vectorizer=vectorizer, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpPDszLpRfww"
   },
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        self.target_size = len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(size={1})>\".format(self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
    "        return {'surname': surname_vector}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, shuffle=True, drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "LDpg2LPKRf0c",
    "outputId": "8f7a26c1-ab27-4b9b-cac2-28937d5d910b"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "surname = input(\"Enter a surname to classify: \")\n",
    "infer_df = pd.DataFrame([surname], columns=['surname'])\n",
    "infer_df.surname = infer_df.surname.apply(preprocess_text)\n",
    "infer_dataset = InferenceDataset(infer_df, vectorizer)\n",
    "results = inference.predict_nationality(dataset=infer_dataset)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQSsKNRSxjRB"
   },
   "source": [
    "# 批量正则化 Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3EamVazx2hx"
   },
   "source": [
    "尽管我们对输入做了标准化，有零平均值和单元方差，帮助收敛。在训练过程中，输入随着穿过不同层和非线性化，也在变化。这也称为内变量移位，它使训练变慢，并且需要更小的学习率。解决方法是[批量正则化](https://arxiv.org/abs/1502.03167)(batchnorm) ，把正则化作为模型架构的一部分。这使得我们可以使用更大的学习率，或的更好的性能，速度更快。\n",
    "\n",
    "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
    "\n",
    "其中：\n",
    "* $a$ = 激活函数 | $\\in \\mathbb{R}^{NXH}$ （$N$ 是样本数， $H$ 是隐含层维度)\n",
    "* $ \\mu_{x}$ = 每个隐含层均值 | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\sigma^2_{x}$ = 每个隐含层方差 | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\epsilon$ = 噪声\n",
    "* $\\gamma$ = 比例参数（已学习的参数）\n",
    "* $\\beta$ = 移位参数 （已学习的参数）\n",
    "\n",
    "\n",
    "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shift and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
    "\n",
    "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
    "\n",
    "where:\n",
    "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
    "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
    "* $\\epsilon$ = noise\n",
    "* $\\gamma$ = scale parameter (learned parameter)\n",
    "* $\\beta$ = shift parameter (learned parameter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9koMITOdzfZB"
   },
   "source": [
    "但是在非线性操作之前，激活函数进行零平均值和单元方差的意义是什么呢？它不是整个激活矩阵都有的属性，只是替代了应用于隐含层维度（本例中是 num_output_channels）的批量正则化。所以计算了每个批次所有样本在每个隐含层的均值和方差。同样的，batchnorm 使用计算的训练中激活函数均值和方差。然而，测试时，因为模型使用训练中保存的均值和方差，可能使模型不准确。PyTorch 的 [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) 会全部自动处理。\n",
    "\n",
    "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_output_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWdAKVEHvyV"
   },
   "outputs": [],
   "source": [
    "# Model with batch normalization\n",
    "class SurnameModel_BN(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
    "        super(SurnameModel_BN, self).__init__()\n",
    "        \n",
    "        # Conv weights\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_output_channels) # define batchnorms\n",
    "                                      for i in range(3)])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "       \n",
    "        # FC weights\n",
    "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        # Conv outputs\n",
    "        z = [F.relu(conv_bn(conv(x))) for conv, conv_bn in zip(self.conv, self.conv_bn)]\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
    "        \n",
    "        # Concat conv outputs\n",
    "        z = torch.cat(z, 1)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # FC layer\n",
    "        y_pred = self.fc1(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "s_QcGx4vN3bQ",
    "outputId": "0371a89e-fd9a-4e31-d5bc-033f1f99f9a9"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(split_df)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel_BN(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                        num_output_channels=args.num_filters,\n",
    "                        num_classes=len(vectorizer.nationality_vocab),\n",
    "                        dropout_p=args.dropout_p)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBXzxtiaxmXi"
   },
   "source": [
    "You can train this model with batch normalization and you'll notice that the validation results improve by ~2-5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ERMGiPgAPssx",
    "outputId": "7fe57758-e8d8-4d90-b2b3-f4b83ae832ee"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "iiAW6AL0QAJ8",
    "outputId": "b287547c-4193-4044-c673-0b1b5ea0d43d"
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GPQH0NVwQAO3",
    "outputId": "2c969a71-a95a-4793-b4cf-7ccec9866643"
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JhAPHwSBZPY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6WRq-O3d1ba"
   },
   "source": [
    "# 待完善 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEcbaRswd1d0"
   },
   "source": [
    "* 图像分类例子\n",
    "* 切片\n",
    "* 深度 CNN 架构\n",
    "* 小的 3X3 滤波器\n",
    "* 填充和步幅的细节（控制接收域，使每个像素作为滤波器中心）\n",
    "* 嵌套网络（1x1 卷积）\n",
    "* 残差连接/残差块\n",
    "* 可解释性（n-grams 火）\n",
    "* image classification example\n",
    "* segmentation\n",
    "* deep CNN architectures\n",
    "* small 3X3 filters\n",
    "* details on padding and stride (control receptive field, make every pixel the center of the filter, etc.)\n",
    "* network-in-network (1x1 conv)\n",
    "* residual connections / residual block\n",
    "* interpretability (which n-grams fire)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "11_Convolutional_Neural_Networks_cn",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
