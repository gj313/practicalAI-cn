{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# 多层感知 Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "<img src=\"../images/logo.png\" width=150>\n",
    "\n",
    "本课程中，我们将学习神经网络的基本类型：多层感知。我们将使用 PyTorch 实现。\n",
    "\n",
    "**注意**： 本文仅是用 PyTorch 对 MLPs 做介绍，因此我们不会按照机器学习技巧去简化（训练/测试拆分时的类平衡，验证集合，及早终止等）。我们将在下一文中实现最佳实践。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# 概览 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "<img src=\"../images/mlp.png\" width=450>\n",
    "\n",
    "$z_2 = XW_1$\n",
    "\n",
    "$a_2 = f(z_2)$\n",
    "\n",
    "$z_3 = a_2W_2$\n",
    "\n",
    "$\\hat{y} = softmax(z_3)$ # 类别\n",
    "\n",
    "*其中*：\n",
    "* $X$ = 输入 | $\\in \\mathbb{R}^{NXD}$ ($D$ 是特征的数量)\n",
    "* $W_1$ = 第1层权重 | $\\in \\mathbb{R}^{DXH}$ ($H$ 是隐含层在第1层的单元数量)\n",
    "* $z_2$ = 第1层权重的输出  $\\in \\mathbb{R}^{NXH}$\n",
    "* $f$ = 非线性激活函数\n",
    "* $a_2$ = 应用在第1层输出的激活值 | $\\in \\mathbb{R}^{NXH}$\n",
    "* $W_2$ = 第2层权重 | $\\in \\mathbb{R}^{HXC}$ ($C$ 是类别数量)\n",
    "* $\\hat{y}$ = 预测 | $\\in \\mathbb{R}^{NXC}$ ($N$ 是采样数量)\n",
    "\n",
    "这是简单的2层 MLP. \n",
    "\n",
    "* **目的：**  基于给定的输入预测类别的概率。因为非线性数据，模型复杂度引入了非线性。\n",
    "* **优势：**\n",
    "  * 能很好的处理非线性模式。\n",
    "* **劣势：**\n",
    "  * 容易过拟合。\n",
    "  * 计算精度取决于网络体积。\n",
    "  * 不容易解释。\n",
    "* **多样性：** 我们之后看到的未来神经网略架构使用 MLP 作为前馈操作（非线性操作之前的仿射变换）的一个模块单元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq65LZJbSpzd"
   },
   "source": [
    "# 训练 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfi_YArvjzrg"
   },
   "source": [
    "*步骤*：\n",
    "\n",
    "1. 随机初始化模型权重 $W$ （我们在之后的课程中将介绍更有效的初始化策略）。\n",
    "2. 给予模型输入 $X$ 得到概率。\n",
    "3. 比较预测值 $\\hat{y}$ （例如  [0.3, 0.3, 0.4]]）和真实值 $y$ （例如 类别2将是 [0, 0, 1]），使用目标（代价）函数决定损失 $J$。分类任务常用的目标函数是交叉熵损失(Cross Entropy Loss)。 \n",
    "  * $z_2 = XW_1$\n",
    "  * $a_2 = max(0, z_2)$ # ReLU 激活函数\n",
    "  * $z_3 = a_2W_2$\n",
    "  * $\\hat{y} = softmax(z_3)$\n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) $\n",
    "4. 计算损失 $J(\\theta)$ w.r.t 对模型权重的梯度。 \n",
    "   * $ \\frac{\\partial{J}}{\\partial{W_{2j}}} = a_2\\hat{y}, \\frac{\\partial{J}}{\\partial{W_{2y}}} = a_2(\\hat{y}-1) $\n",
    "   * $ \\frac{\\partial{J}}{\\partial{W_1}} = \\frac{\\partial{J}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{a_2}}  \\frac{\\partial{a_2}}{\\partial{z_2}}  \\frac{\\partial{z_2}}{\\partial{W_1}}  = W_2(\\partial{scores})(\\partial{ReLU})X $\n",
    "   \n",
    "5. 应用反向传播法（backpropagation）到使用梯度下降更新权重 $W$ 。这些更新将减少错误类别（j）的概率，增加正确类别（y）的概率。\n",
    "  * $W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}}$\n",
    "6. 重复步骤2-4，直到模型收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtKqNioAayCy"
   },
   "source": [
    "# 数据 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3OrtMpFayFC"
   },
   "source": [
    "我们将为分类任务生成一些非线性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvkfS3JZOMgB"
   },
   "outputs": [],
   "source": [
    "# Load PyTorch library\n",
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NfIz_4OPYpG"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdcWUP-tTHj0"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    num_samples_per_class=500,\n",
    "    dimensions=2,\n",
    "    num_classes=3,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_hidden_units=100,\n",
    "    learning_rate=1e-0,\n",
    "    regularization=1e-3,\n",
    "    num_epochs=200,\n",
    ")\n",
    "\n",
    "# Set seed for reproducability\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2O38IuNayR5"
   },
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "def generate_data(num_samples_per_class, dimensions, num_classes):\n",
    "    # Make synthetic spiral data\n",
    "    X_original = np.zeros((num_samples_per_class*num_classes, dimensions))\n",
    "    y = np.zeros(num_samples_per_class*num_classes, dtype='uint8')\n",
    "    for j in range(num_classes):\n",
    "        ix = range(num_samples_per_class*j,num_samples_per_class*(j+1))\n",
    "        r = np.linspace(0.0,1,num_samples_per_class) # radius\n",
    "        t = np.linspace(j*4,(j+1)*4,num_samples_per_class) + \\\n",
    "        np.random.randn(num_samples_per_class)*0.2 # theta\n",
    "        X_original[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "\n",
    "    # Stack\n",
    "    X = np.hstack([X_original])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q14QCqKxUS2v",
    "outputId": "78bc9bff-0db3-41ee-8297-7377c20809a8"
   },
   "outputs": [],
   "source": [
    "# Generate X & y\n",
    "X, y = generate_data(num_samples_per_class=args.num_samples_per_class, \n",
    "                     dimensions=args.dimensions, num_classes=args.num_classes)\n",
    "print (\"X: {0}\".format(np.shape(X)))\n",
    "print (\"y: {0}\".format(np.shape(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "jgVjStv8VnX2",
    "outputId": "5eb7823c-34fd-4587-b9f0-c24f7e06d2dd"
   },
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "plt.title(\"Generated non-linear data\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XImjkyN1MZn"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gGFqcqTDXhkl",
    "outputId": "4b9f6fd1-3d85-4015-d53f-eb89daad04a7"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "\n",
    "# Split datasets\n",
    "test_start_idx = int(len(X) * args.train_size)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHofozO7RIiV"
   },
   "source": [
    "# 线性模型 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlVmr5XkRMCf"
   },
   "source": [
    "在使用神经网络之前，我们先用 PyTorch 的线性模型（逻辑回归）试一下。我们想看看为什么线性模型不能满足数据要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qja6kvBrRKDj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AdXiZ8ORKGS"
   },
   "outputs": [],
   "source": [
    "# Linear model\n",
    "class LogisticClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LogisticClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = self.fc1(x_in)\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ApoN49qmRhl5",
    "outputId": "0d06b9d4-3a94-4c45-81a1-d6d302734714"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LogisticClassifier(input_dim=args.dimensions, \n",
    "                           hidden_dim=args.num_hidden_units, \n",
    "                           output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ksXe8ruRhoh"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) # Adam optimizer (usually better than SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bt3kwn_3kso9"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(y_pred, y_target):\n",
    "    n_correct = torch.eq(y_pred, y_target).sum().item()\n",
    "    accuracy = n_correct / len(y_pred) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "cpo8c46ERhrS",
    "outputId": "06927f18-a3f3-4738-e94c-32b32fc3fd45"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%20==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | acc: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZthV18sPRhto"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZjKHD3zXbb0I",
    "outputId": "f6e04324-9582-4a5f-f7c9-fe5e44181126"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hsn8zbxRh09"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    cmap = plt.cm.Spectral\n",
    "    \n",
    "    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
    "    y_pred = model(X_test, apply_softmax=True)\n",
    "    _, y_pred = y_pred.max(dim=1)\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "5u5fEhOcRh3V",
    "outputId": "9ebec4e7-f6f1-487c-b451-9493ffd138d9"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llXUlx5bRKJB"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC9Hqrl7RxVr"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "tlwsKnBhRxYH",
    "outputId": "26622315-45e5-44dc-8a2c-0896289eae18"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "br_O_NUCjkwm"
   },
   "source": [
    "# 非线性模型 Non-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YexKO17makzj"
   },
   "source": [
    "现在，我们按一下 MLP 如何工作的。注意：和之前唯一的区别是增加了非线性的激活函数（我们使用 ReLU $max(0, z)）。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-U0_3Xco0Q4g"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) # activaton function added!\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "V-8S6w1njkDC",
    "outputId": "5ed4037a-7116-4f86-c989-906a8605bad2"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, \n",
    "            hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtZvQVVpvl5p"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "nmeU45XF0O-U",
    "outputId": "90bf9112-a477-437a-d65a-b59349498b20"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%20==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | acc: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4tI2iZ1vl8e"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CgM4qu8I62BP",
    "outputId": "b38195c6-4b00-44b8-9ed0-3e07657ab1e7"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "Xj7cwzZ4NoVI",
    "outputId": "cdee010c-f955-4632-feda-c4038e0b9baa"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "IsxhNLTr_Jg7",
    "outputId": "5e44b5b5-c6fd-4c2d-bfdd-1914734e33c5"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-TgqRKOOSUw"
   },
   "source": [
    "# 权重可视化 Visualizing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7F2Ye5hOSYr"
   },
   "source": [
    "截至现在，我们看到了损失（loss）和精度（accuracy）等指标，甚至决策边界的可视化。但是我们的权重呢？因为他们数量太多，且在一直变化，所以可视化起来有些复杂。但这是必要的，因为权重会引起很多下游问题，例如所有权重趋近于0或者权重数量级快速增长。这些问题表示我们的模型需要一些调整/标准化。我们要能噶偶看到权重的这些变化。因为权重有维度很大，我们也需要不同参数均值和标准差的可视化。\n",
    "\n",
    "我们使用 [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) 和 PyTorch 可视化所有内容。Tensorboard 可以用于本地可视化，但是在  Google colab 上有些复杂，所以我们使用一个 localtunnel 显示 notebook 服务器。如果你在本地电脑进行，只需要在终端运行 `tensorboard --logdir='./logs' --port=6006` ，在`http://localhost:6006`打开TensorBoard。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/tensorboard.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7GfEoEWhExI"
   },
   "source": [
    "以下是易于测量和可视化参数：\n",
    "1. 损失和精度\n",
    "2. 权重均值和标准差\n",
    "3. 激活函数均值和标准差\n",
    "4. 梯度均值和标定差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "jilLRr8W05qC",
    "outputId": "e5d9238b-7644-41d4-e229-620afdcbc935"
   },
   "outputs": [],
   "source": [
    "# Install TensorboardX\n",
    "!pip3 install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6rsfwPuRRMC"
   },
   "outputs": [],
   "source": [
    "# Run tensorboard on port 6006\n",
    "LOG_DIR = './log'\n",
    "run_num = 0\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "FwlrVIikRhzt",
    "outputId": "9f81084b-170a-4aff-d289-35bf660f3e4b"
   },
   "outputs": [],
   "source": [
    "# Install localtunnel\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG64Zj7gRjz2"
   },
   "outputs": [],
   "source": [
    "# Tunnel port 6006 for tensorboard\n",
    "get_ipython().system_raw('lt --port 6006 >> tensorboard.txt 2>&1 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efEPZIuSQJKP"
   },
   "source": [
    "现在我们开始训练模型，并在 tensorboard 做些可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "Q4L_Um9MP3EW",
    "outputId": "84617b02-a1d5-4d4a-c4d0-a125455965c3"
   },
   "outputs": [],
   "source": [
    "# Few things needed to get tensorboard working\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "!pip install Pillow==4.0.0\n",
    "!pip install PIL\n",
    "!pip install image\n",
    "from PIL import Image\n",
    "def register_extension(id, extension): Image.EXTENSION[extension.lower()] = id.upper()\n",
    "Image.register_extension = register_extension\n",
    "def register_extensions(id, extensions): \n",
    "    for extension in extensions: register_extension(id, extension)\n",
    "Image.register_extensions = register_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUsGMQliVf6j"
   },
   "outputs": [],
   "source": [
    "# Initialize the Tensorboard writer\n",
    "run_num += 1\n",
    "writer = SummaryWriter(log_dir=LOG_DIR+\"/run_{}\".format(run_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "VGLYVpplWTqK",
    "outputId": "69a02442-17b5-4131-c9c2-1aa65ad312e4"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, \n",
    "            hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8hWwoCUWTwG"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DshX8a8qaa1q"
   },
   "outputs": [],
   "source": [
    " def write_weights(writer, model, epoch_num):\n",
    "    for name, param in model.named_parameters():\n",
    "        \n",
    "        # Weights\n",
    "        writer.add_scalar(name+\"/mean\", param.data.numpy().mean(), epoch_num)\n",
    "        writer.add_scalar(name+\"/std\", param.data.numpy().std(), epoch_num)\n",
    "        \n",
    "        # Gradients\n",
    "        writer.add_scalar(name+\"/grad_mean\", torch.mean(param.grad), epoch_num)\n",
    "        writer.add_scalar(name+\"/grad_std\", torch.std(param.grad), epoch_num)\n",
    "        \n",
    "        # Weights histogram (dim over 1024 cause an error)\n",
    "        if len(param.size()) > 1 and param.size()[-1] <= 1024: \n",
    "            writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "A336qkQxWTuP",
    "outputId": "8fd6624d-bdd6-47e0-a5ba-fa7bd3584167"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    \n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%20==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | accuracy: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Write to tensorboard\n",
    "    writer.add_scalar('metrics/train_loss', loss, t)\n",
    "    writer.add_scalar('metrics/train_acc', accuracy, t)\n",
    "    writer.add_scalar('metrics/lr', optimizer.param_groups[0]['lr'], t)\n",
    "    write_weights(writer=writer, model=model, epoch_num=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "j-IEhlPYVf38",
    "outputId": "39b42566-7d01-4f1e-9f5c-31334cf4183a"
   },
   "outputs": [],
   "source": [
    "print (\"Go to this link below to see the Tensorboard:\")\n",
    "!cat tensorboard.txt\n",
    "print (\"Click on SCALARS to see metrics and DISTRIBUTIONS to see weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3OK8p-Ng3BC"
   },
   "source": [
    "# 激活函数 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghf5uLuhg3D0"
   },
   "source": [
    "在我们的 MPL 中，我们使用了 ReLU 激活函数（$max(0,z))），这是目前最常用的。当然还有其他几种激活函数，各有特点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "ivnfSKEhg3Md",
    "outputId": "45b7cb4b-a87c-4bf5-dd81-4c3b1ef381ee"
   },
   "outputs": [],
   "source": [
    "# Fig size\n",
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "# Data\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "\n",
    "# Sigmoid activation (constrain a value between 0 and 1.)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Sigmoid activation\")\n",
    "y = torch.sigmoid(x)\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Tanh activation (constrain a value between -1 and 1.)\n",
    "plt.subplot(1, 3, 2)\n",
    "y = torch.tanh(x)\n",
    "plt.title(\"Tanh activation\")\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Relu (clip the negative values to 0)\n",
    "plt.subplot(1, 3, 3)\n",
    "y = F.relu(x)\n",
    "plt.title(\"ReLU activation\")\n",
    "plt.plot(x.numpy(), y.numpy())\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWv-RU46k2UL"
   },
   "source": [
    "# 初始化权重 Initializing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hDPBE0sk2mJ"
   },
   "source": [
    "目前为止，我们使用小随机数做了权重初始化，但不是模型训练收敛的最优值。目标是得到一个在所有神经元都能有相似分布输出的权重。我们可以采用相比仿射和非线性操作，更优先考虑单位方差的权重。\n",
    "常用的方法是使用 [xavier 初始化](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)，可以使信号从数据到每一层次网络。我们将在 PyTorch 模型中使用这个方法。\n",
    "你可能会问：为什么我们不在每一个前向传播中使用？这是一个好问题。我们将在后续课程中学习更高级的优化方法，例如 批量/层正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOYptFo7k-JI"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu')) \n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) # activaton function added!\n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ijvHwcZg8mO"
   },
   "source": [
    "# 过拟合 Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIhvdD_zg8os"
   },
   "source": [
    "虽然神经网络很善于处理非线性数据关系，也容易在测试数据上过拟合，不能通过测试数据。如下面例子所示，我们生成完全随机的数据，能够拟合一个有 [$2*N*C + D$](https://arxiv.org/abs/1611.03530)  隐含层的模型。训练结果非常好，但是过拟合导致在测试数据表现很差。我们将在后续课程中学习解决过拟合的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRdM16NhazJP"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    num_samples_per_class=40,\n",
    "    dimensions=2,\n",
    "    num_classes=3,\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_hidden_units=2*40*3+2 , # 2*N*C + D\n",
    "    learning_rate=1e-3,\n",
    "    regularization=1e-3,\n",
    "    num_epochs=1000,\n",
    ")\n",
    "\n",
    "# Set seed for reproducability\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qf00Biq6g8ty",
    "outputId": "fb7c0f5c-b15e-402e-ebce-a0260118aaea"
   },
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "X = torch.randn(args.num_samples_per_class*args.num_classes, args.dimensions).float()\n",
    "y = torch.LongTensor([[i]*args.num_samples_per_class \n",
    "                       for i in range(args.num_classes)]).view(-1)\n",
    "print (\"X: {0}\".format(np.shape(X)))\n",
    "print (\"y: {0}\".format(np.shape(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-bA9vK9SWkjh",
    "outputId": "4d304402-2504-4c3d-9388-5260c0868df0"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "\n",
    "# Split datasets\n",
    "test_start_idx = int(len(X) * args.train_size)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-_8b7AlaFdY"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        print \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        a_1 = F.relu(self.fc1(x_in)) \n",
    "        y_pred = self.fc2(a_1)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xozz2bBoWkmq",
    "outputId": "be6b5de8-f06e-4407-c872-5b5fd3b6d164"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXnkWoPaWkpe"
   },
   "outputs": [],
   "source": [
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "zgayj4E1WksH",
    "outputId": "ebba47f1-25de-4a51-f6d8-cda697c500ea"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for t in range(args.num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Accuracy\n",
    "    _, predictions = y_pred.max(dim=1)\n",
    "    accuracy = get_accuracy(y_pred=predictions.long(), y_target=y_train)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # Verbose\n",
    "    if t%100==0: \n",
    "        print (\"epoch: {0:02d} | loss: {1:.4f} | accuracy: {2:.1f}%\".format(\n",
    "            t, loss, accuracy))\n",
    "\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3OJLNwuZxtk"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "_, pred_train = model(X_train, apply_softmax=True).max(dim=1)\n",
    "_, pred_test = model(X_test, apply_softmax=True).max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_LU9Wzt0ZxwI",
    "outputId": "0586e063-e940-495b-b872-ace35455a4cb"
   },
   "outputs": [],
   "source": [
    "# Train and test accuracies\n",
    "train_acc = get_accuracy(y_pred=pred_train, y_target=y_train)\n",
    "test_acc = get_accuracy(y_pred=pred_test, y_target=y_test)\n",
    "print (\"train acc: {0:.1f}%, test acc: {1:.1f}%\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "rpSoAEdGWku5",
    "outputId": "85df95a1-c511-407b-f28e-e92cbc08a214"
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_train, y=y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_multiclass_decision_boundary(model=model, X=X_test, y=y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "_4m9TXpTZ22C",
    "outputId": "bab12967-1565-49e0-b139-06d67b803fed"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[0, 1, 2])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAyjh3bieLjn"
   },
   "source": [
    "# 丢弃 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6kLwcBveLy5"
   },
   "source": [
    "增加数据是解决过拟合非常好的方法，但通常不可行。幸运的是，有正则化和 dropout 可以帮助我们床一个鲁棒性更好的模型。我们已经介绍过正则化，可以很方便的添加到 PyTorch 模型中。\n",
    "dropout（只用于训练）允许我们消除神经元的输出。我们用在每一层 p% 的神经元，这个概率每批都变。dropout 避免了神经单元和数据的共适应，类似于一个采样策略，每次丢弃了一个不同的神情元组合。\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/dropout.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGQq0MvcgBEG"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args.dropout_p = 0.1 # 40% of the neurons are dropped each pass\n",
    "args.lambda_l2 = 1e-4 # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6NvhBUyf27y"
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p) # Defining the dropout\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.xavier_normal(self.fc1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        z = F.relu(self.fc1(x_in))\n",
    "        z = self.dropout(z) # dropping neurons\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "XQK9h7BNf26K",
    "outputId": "3cc6e22b-2c63-4799-fac9-96673f4c7fbe"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MLP(input_dim=args.dimensions, \n",
    "            hidden_dim=args.num_hidden_units, \n",
    "            output_dim=args.num_classes, \n",
    "            dropout_p=args.dropout_p)\n",
    "print (model.named_modules)\n",
    "\n",
    "# Optimization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, \n",
    "                       weight_decay=args.lambda_l2) # Adding L2 regularization\n",
    "\n",
    "# Training\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0aQUomQoni1"
   },
   "source": [
    "# 其他资源 Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzYGBzEWV9a3"
   },
   "source": [
    "- interpretability (easy w/ at with binary tasks)\n",
    "- dropconnect (but not really used)\n",
    "- PReLU 激活函数"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_Multilayer_Perceptron-cn",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
