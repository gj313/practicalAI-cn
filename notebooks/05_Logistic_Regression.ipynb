{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOChJSNXtC9g"
   },
   "source": [
    "# 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ"
   },
   "source": [
    "<img src=\"../images/logo.png\" width=150>\n",
    "\n",
    "在上一节中，我们看到线性回归可以很好的拟合出一条线后者一个超平面来做出对连续变量的预测。但是在分类问题中我们希望的输出是类别的概率，线性回归就不能做的很好了。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VoMq0eFRvugb"
   },
   "source": [
    "# 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL"
   },
   "source": [
    "<img src=\"../images/logistic.jpg\" width=270>\n",
    "\n",
    "$ \\hat{y} = \\frac{1}{1 + e^{-XW}} $ \n",
    "\n",
    "*where*:\n",
    "* $\\hat{y}$ = 预测值 | $\\in \\mathbb{R}^{NX1}$ ($N$ 是样本的个数)\n",
    "* $X$ = 输入 | $\\in \\mathbb{R}^{NXD}$ ($D$ 是特征的个数)\n",
    "* $W$ = 权重 | $\\in \\mathbb{R}^{DX1}$ \n",
    "\n",
    "这个是二项式逻辑回归。主要思想是用线性回归的输出值($z=XW$)经过一个sigmoid函数($\\frac{1}{1+e^{-z}}$)来映射到(0, 1)之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcFvkklZSZr9"
   },
   "source": [
    "当我们有多于两个分类类别，我们就需要使用多项式逻辑回归(softmax分类器)。softmax分类器将会用线性方程($z=XW$)并且归一化它，来产生对应的类别y的概率。\n",
    "\n",
    "$ \\hat{y} = \\frac{e^{XW_y}}{\\sum e^{XW}} $ \n",
    "\n",
    "*where*:\n",
    "* $\\hat{y}$ = 预测值 | $\\in \\mathbb{R}^{NX1}$ ($N$ 是样本的个数)\n",
    "* $X$ = 输入 | $\\in \\mathbb{R}^{NXD}$ ($D$ 是特征的个数)\n",
    "* $W$ = 权重 | $\\in \\mathbb{R}^{DXC}$ ($C$ 是类别的个数)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4Y55tpzIjOa"
   },
   "source": [
    "* **目标:**  通过输入值$X$来预测$y$的类别概率。softmax分类器将根据归一化线性输出来计算类别概率。 \n",
    "* **优点:**\n",
    "  * 可以预测与输入对应的类别概率。\n",
    "* **缺点:**\n",
    "  * 因为使用的损失函数是要最小化交叉熵损失，所以对离群点很敏感。(支持向量机([SVMs](https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f)) 是对处理离群点一个很好的选择).\n",
    "* **其他:** Softmax分类器在神经网络结构中广泛用于最后一层，因为它会计算出类别的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq65LZJbSpzd"
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HBPn8zPTQfZ"
   },
   "source": [
    "*步骤*:\n",
    "\n",
    "1. 随机初始化模型权重$W$.\n",
    "2. 将输入值 $X$ 传入模型并且得到logits ($z=XW$). 在logits上使用softmax操作得到独热编码后的类别概率$\\hat{y}$。 比如, 如果有三个类别, 预测出的类别概率可能为[0.3, 0.3, 0.4]. \n",
    "3. 使用损失函数将预测值$\\hat{y}$ (例如[0.3, 0.3, 0.4]])和真实值$y$(例如属于第二个类别应该写作[0, 0, 1])做对比，并且计算出损失值$J$。一个很常用的逻辑回归损失函数是交叉熵函数。 \n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) $\n",
    "   * $y$ = [0, 0, 1]\n",
    "  * $\\hat{y}$ = [0.3, 0.3, 0.4]]\n",
    "  * $J(\\theta) = - \\sum_i y_i ln (\\hat{y_i}) =  - \\sum_i y_i ln (\\frac{e^{X_iW_y}}{\\sum e^{X_iW}}) = - \\sum_i [0 * ln(0.3) + 0 * ln(0.3) + 1 * ln(0.4)] = -ln(0.4) $\n",
    "  * 简化我们的交叉熵函数: $J(\\theta) = - ln(\\hat{y_i})$ (负的最大似然).\n",
    "  * $J(\\theta) = - ln(\\hat{y_i}) = - ln (\\frac{e^{X_iW_y}}{\\sum_i e^{X_iW}}) $\n",
    "4. 根据模型权重计算损失梯度$J(\\theta)$。让我们假设类别的分类是互斥的(一种输入仅仅对应一个输出类别).\n",
    " * $\\frac{\\partial{J}}{\\partial{W_j}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_j}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}0 - e^{W_yX}e^{W_jX}X}{(\\sum e^{XW})^2} = \\frac{Xe^{W_j}X}{\\sum e^{XW}} = XP$\n",
    "  * $\\frac{\\partial{J}}{\\partial{W_y}} = \\frac{\\partial{J}}{\\partial{y}}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{y}\\frac{\\partial{y}}{\\partial{W_y}} = - \\frac{1}{\\frac{e^{W_yX}}{\\sum e^{XW}}}\\frac{\\sum e^{XW}e^{W_yX}X - e^{W_yX}e^{W_yX}X}{(\\sum e^{XW})^2} = \\frac{1}{P}(XP - XP^2) = X(P-1)$\n",
    "5. 使用梯度下降法来对权重做反向传播以更新模型权重。更新后的权重将会使不正确的类别(j)概率大大降低，从而升高正确的类别(y)概率。\n",
    "  * $W_i = W_i - \\alpha\\frac{\\partial{J}}{\\partial{W_i}}$\n",
    "6. 重复2 - 4步直到模型表现最好（也可以说直到损失收敛）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_hKrjzdtTgM"
   },
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyccHrQztVEu"
   },
   "source": [
    "我们来加载在第三节课中用到的titanic数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H385V4VUtWOv"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pL67TlZO6Zg4"
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"../data/titanic.csv\",\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "# 设置随即种子来保证实验结果的可重复性。\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sp_tSyItf1_"
   },
   "outputs": [],
   "source": [
    "# # 从GitHub上加载数据到notebook本地驱动\n",
    "# url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/titanic.csv\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# html = response.read()\n",
    "# with open(args.data_file, 'wb') as f:\n",
    "#     f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "7alqmyzXtgE8",
    "outputId": "353702e3-76f7-479d-df7a-5effcc8a7461"
   },
   "outputs": [],
   "source": [
    "# 把CSV文件内容读到DataFrame中\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-5Y4zLIoE6s"
   },
   "source": [
    "# Scikit-learn实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILkbyBHQoIwE"
   },
   "source": [
    "**注意**: Scikit-learn中`LogisticRegression`类使用的是坐标下降法（coordinate descent）来做的拟合。然而，我们会使用Scikit-learn中的`SGDClassifier`类来做随机梯度下降。我们使用这个优化方法是因为在未来的几节课程中我们也会使用到它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1MJODStIu8V"
   },
   "outputs": [],
   "source": [
    "# 调包\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kItBIOOCTi6p"
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess(df):\n",
    "  \n",
    "    # 删除掉含有空值的行\n",
    "    df = df.dropna()\n",
    "\n",
    "    # 删除基于文本的特征 (我们以后的课程将会学习怎么使用它们)\n",
    "    features_to_drop = [\"name\", \"cabin\", \"ticket\"]\n",
    "    df = df.drop(features_to_drop, axis=1)\n",
    "\n",
    "    # pclass, sex, 和 embarked 是类别变量\n",
    "    categorical_features = [\"pclass\",\"embarked\",\"sex\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "QwQHDh4xuYTB",
    "outputId": "153ea757-b817-406d-dbde-d1fba88f194b"
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "df = preprocess(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wsGRZNNiUTqj",
    "outputId": "c9364be7-3cae-487f-9d96-3210b3129199"
   },
   "outputs": [],
   "source": [
    "# 划分数据到训练集和测试集\n",
    "mask = np.random.rand(len(df)) < args.train_size\n",
    "train_df = df[mask]\n",
    "test_df = df[~mask]\n",
    "print (\"Train size: {0}, test size: {1}\".format(len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZKxFmATU95M"
   },
   "source": [
    "**注意**: 如果你有类似标准化的预处理步骤，你需要在划分完训练集和测试集之后再使用它们。这是因为我们不可能从测试集中学到任何有用的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLzL_LJd4vQ-"
   },
   "outputs": [],
   "source": [
    "# 分离 X 和 y\n",
    "X_train = train_df.drop([\"survived\"], axis=1)\n",
    "y_train = train_df[\"survived\"]\n",
    "X_test = test_df.drop([\"survived\"], axis=1)\n",
    "y_test = test_df[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AdTYbV472UNJ",
    "outputId": "214a8114-3fd3-407f-cd6e-5f5d07294f50"
   },
   "outputs": [],
   "source": [
    "# 标准化训练数据 (mean=0, std=1)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# 标准化训练和测试数据  (不要标准化标签分类y)\n",
    "standardized_X_train = X_scaler.transform(X_train)\n",
    "standardized_X_test = X_scaler.transform(X_test)\n",
    "\n",
    "# 检查\n",
    "print (\"mean:\", np.mean(standardized_X_train, axis=0)) # mean 应该为 ~0\n",
    "print (\"std:\", np.std(standardized_X_train, axis=0))   # std 应该为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-vm9AZm1_f9"
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "log_reg = SGDClassifier(loss=\"log\", penalty=\"none\", max_iter=args.num_epochs, \n",
    "                        random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0e8U9NNluYVp",
    "outputId": "c5f22ade-bb8c-479b-d300-98758a82d396"
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "log_reg.fit(X=standardized_X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "hA7Oz97NAe8A",
    "outputId": "ab8a878a-6012-4727-8cd1-40bc5c69245b"
   },
   "outputs": [],
   "source": [
    "# 概率\n",
    "pred_test = log_reg.predict_proba(standardized_X_test)\n",
    "print (pred_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-jZtTd7F6_ps",
    "outputId": "d2306e4c-88a4-4ac4-9ad5-879fa461617f"
   },
   "outputs": [],
   "source": [
    "# 预测 (未标准化)\n",
    "pred_train = log_reg.predict(standardized_X_train) \n",
    "pred_test = log_reg.predict(standardized_X_test)\n",
    "print (pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dM7iYW8ANYjy"
   },
   "source": [
    "# 评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFXbczqu8Rno"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sEjansj78Rqe",
    "outputId": "f5bfbe87-12c9-4aa5-fc61-e615ad4e63d4"
   },
   "outputs": [],
   "source": [
    "# 正确率\n",
    "train_acc = accuracy_score(y_train, pred_train)\n",
    "test_acc = accuracy_score(y_test, pred_test)\n",
    "print (\"train acc: {0:.2f}, test acc: {1:.2f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WijzY-vDNbE9"
   },
   "source": [
    "到目前为止我们用的是正确率作为我们的评价指标来评定模型的好坏程度。但是我们还有很多的评价指标来对模型进行评价。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/metrics.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80MwyE0yOr-k"
   },
   "source": [
    "评价指标的选择真的要看应用的情景。\n",
    "positive - true, 1, tumor, issue, 等等, negative - false, 0, not tumor, not issue, 等等。\n",
    "\n",
    "$\\text{accuracy}（正确率） = \\frac{TP+TN}{TP+TN+FP+FN}$ \n",
    "\n",
    "$\\text{recall}（召回率）= \\frac{TP}{TP+FN}$ → (有多个正例被我分为正例)\n",
    "\n",
    "$\\text{precision} （精确率）= \\frac{TP}{TP+FP}$ → (在所有我预测为正例的样本下，有多少是对的)\n",
    "\n",
    "$F_1 = 2 * \\frac{\\text{precision }  *  \\text{ recall}}{\\text{precision } + \\text{ recall}}$\n",
    "\n",
    "where: \n",
    "* TP: 将正类预测为正类数\n",
    "* TN: 将负类预测为负类数\n",
    "* FP: 将负类预测为正类数\n",
    "* FN: 将正类预测为负类数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opmu3hJm9LXA"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAzOL8h29m82"
   },
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "KqUVzahQ-5ic",
    "outputId": "bff8819e-3d5b-45b9-c221-179c873140b1"
   },
   "outputs": [],
   "source": [
    "# 混淆矩阵\n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plot_confusion_matrix(cm=cm, classes=[\"died\", \"survived\"])\n",
    "print (classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMk7tN1h98x9"
   },
   "source": [
    "当我们有大于两个标签（二分类）的时候，我们可以选择在微观/宏观层面计算评估指标（每个clas标签）、权重等。 更详细内容可以参考[offical docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v6zc1_1PWnz"
   },
   "source": [
    "# 推论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zl9euDuMPYTN"
   },
   "source": [
    "现在我们来看看你是否会在Titanic中存活下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "kX9428-EPUzx",
    "outputId": "ef100af7-9861-4900-e9c7-ed6d93c69069"
   },
   "outputs": [],
   "source": [
    "# 输入你自己的信息\n",
    "X_infer = pd.DataFrame([{\"name\": \"Goku Mohandas\", \"cabin\": \"E\", \"ticket\": \"E44\", \n",
    "                         \"pclass\": 1, \"age\": 24, \"sibsp\": 1, \"parch\": 2, \n",
    "                         \"fare\": 100, \"embarked\": \"C\", \"sex\": \"male\"}])\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "c6OAAQoaWxAb",
    "outputId": "85eb1c6d-6f53-4bd4-bcc3-90d9ebca74c8"
   },
   "outputs": [],
   "source": [
    "# 进行预处理\n",
    "X_infer = preprocess(X_infer)\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "48sj5A0mX5Yw",
    "outputId": "d9571238-70ab-427d-f80c-7b13b00efc95"
   },
   "outputs": [],
   "source": [
    "# 添加缺失列向量\n",
    "missing_features = set(X_test.columns) - set(X_infer.columns)\n",
    "for feature in missing_features:\n",
    "    X_infer[feature] = 0\n",
    "\n",
    "# 重整title\n",
    "X_infer = X_infer[X_train.columns]\n",
    "X_infer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rP_i8w9IXFiM"
   },
   "outputs": [],
   "source": [
    "# 标准化\n",
    "standardized_X_infer = X_scaler.transform(X_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7O5PbOAvXTzF",
    "outputId": "f1c3597e-1676-476f-e970-168e5c3fca6c"
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_infer = log_reg.predict_proba(standardized_X_infer)\n",
    "classes = {0: \"died\", 1: \"survived\"}\n",
    "_class = np.argmax(y_infer)\n",
    "print (\"Looks like I would've {0} with about {1:.0f}% probability on the Titanic expedition!\".format(\n",
    "    classes[_class], y_infer[0][_class]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PLPFFP67tvL"
   },
   "source": [
    "# 可解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jv6LKNXO7uch"
   },
   "source": [
    "哪些特征是最有影响力的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KTSpxbwy7ugl",
    "outputId": "b37bf39c-f35d-4793-a479-6e61179fc5e5"
   },
   "outputs": [],
   "source": [
    "# 未标准化系数\n",
    "coef = log_reg.coef_ / X_scaler.scale_\n",
    "intercept = log_reg.intercept_ - np.sum((coef * X_scaler.mean_))\n",
    "print (coef)\n",
    "print (intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJgiIupyE0Hd"
   },
   "source": [
    "正系数表示与阳性类的相关性（1 = 存活），负系数表示与阴性类的相关性（0 = 死亡）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RKRB0er2C5l-",
    "outputId": "39ad0cf3-13b1-4aa8-9a6b-4456b8975a39"
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(coef)\n",
    "features = list(X_train.columns)\n",
    "print (\"Features correlated with death:\", [features[i] for i in indices[0][:3]])\n",
    "print (\"Features correlated with survival:\", [features[i] for i in indices[0][-3:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhhFw3Kg-4aL"
   },
   "source": [
    "### 非标准化系数的证明:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ER0HFHXj-4h8"
   },
   "source": [
    "注意我们的X和y都已经标准化了。\n",
    "\n",
    "$\\mathbb{E}[y] = W_0 + \\sum_{j=1}^{k}W_jz_j$\n",
    "\n",
    "$z_j = \\frac{x_j - \\bar{x}_j}{\\sigma_j}$\n",
    "\n",
    "$ \\hat{y} = \\hat{W_0} + \\sum_{j=1}^{k}\\hat{W_j}z_j $\n",
    "\n",
    "$\\hat{y} = \\hat{W_0} + \\sum_{j=1}^{k} \\hat{W}_j (\\frac{x_j - \\bar{x}_j}{\\sigma_j}) $\n",
    "\n",
    "$\\hat{y} = (\\hat{W_0} - \\sum_{j=1}^{k} \\hat{W}_j\\frac{\\bar{x}_j}{\\sigma_j}) +  \\sum_{j=1}^{k} (\\frac{\\hat{w}_j}{\\sigma_j})x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yBZLVHwGKSj"
   },
   "source": [
    "# K折交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHyLTMAAGJ_x"
   },
   "source": [
    "交叉验证是一个重采样的模型评估方法。与其我们在一开始就仅仅划分一次训练集和验证集，我们用交叉验证来划分k(通常 k=5 或者 10)次不同的训练集和验证集。\n",
    "\n",
    "步骤:\n",
    "1.   随机打乱训练数据集*train*。\n",
    "2.   将数据集分割成不同的k个片段。\n",
    "3.   在k次的每次循环中选择一个片段来当作验证集，其余的所有片段当成训练集。\n",
    "4.   重复这个过程使每个片段都有可能成为训练集或者测试集的一部分。\n",
    "5.   随机初始化权重来训练模型。\n",
    "6.   在k个循环中每次都要重新初始化模型，但是权重要保持相同的随机初始化，然后再在验证集中进行验证。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XB6X1b0KcvJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIqKmAEtVWMg"
   },
   "outputs": [],
   "source": [
    "#  K折交叉验证\n",
    "log_reg = SGDClassifier(loss=\"log\", penalty=\"none\", max_iter=args.num_epochs)\n",
    "scores = cross_val_score(log_reg, standardized_X_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0aQUomQoni1"
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCpKSu53EA9-"
   },
   "source": [
    "- interaction terms\n",
    "- interpreting odds ratio\n",
    "- simple example with coordinate descent method (sklearn.linear_model.LogisticRegression)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_Logistic_Regression",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
